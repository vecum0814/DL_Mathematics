{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dense Layers","metadata":{}},{"cell_type":"markdown","source":"## Shapes of Dense Layers","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\n\n\nN, n_features = 8, 10\nx = torch.randn(N, n_features)\n\nn_neuron = 3\n\nclass Dense(torch.nn.Module):\n    def __init__(self, n_neuron = 3):\n        super(Dense, self).__init__()\n        self.layer = nn.Linear(10, n_neuron)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.layer(x)\n        x = self.sigmoid(x)\n        return x\n    \n\nmodel = Dense()\nY = model(x)\n\nw = list(model.parameters()) # get the values of weight, bias\nW, B = w[0], w[1]\n\nprint('==== Input/Weights/Bias =====')\nprint('x: ', x.shape)\nprint('W: ', W.shape)\nprint('B: ', B.shape)\nprint('Y: ', Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:37:03.367983Z","iopub.execute_input":"2022-06-10T11:37:03.368814Z","iopub.status.idle":"2022-06-10T11:37:03.380459Z","shell.execute_reply.started":"2022-06-10T11:37:03.368777Z","shell.execute_reply":"2022-06-10T11:37:03.379430Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"==== Input/Weights/Bias =====\nx:  torch.Size([8, 10])\nW:  torch.Size([3, 10])\nB:  torch.Size([3])\nY:  torch.Size([8, 3])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Output Calculations","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\n\n\nN, n_features = 4, 10\nx = torch.randn(N, n_features)\n\nn_neuron = 3\n\nclass Dense(torch.nn.Module):\n    def __init__(self, n_neuron = 3):\n        super(Dense, self).__init__()\n        self.layer = nn.Linear(10, n_neuron)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.layer(x)\n        x = self.sigmoid(x)\n        return x\n    \n\nmodel = Dense()\nY_torch = model(x)\nprint('Y(Torch): \\n', Y_torch.detach().numpy())\n\n# Calculation with matrix multiplication\n\nw = list(model.parameters()) # get the values of weight, bias\nW, B = w[0], w[1]\n\nz = torch.matmul(x, torch.transpose(W, 0, 1)) + B\nY_manual = 1 / (1 + torch.exp(-z))\n\n\nprint('Y(Manual): \\n', Y_manual.detach().numpy())\n\n# Calculation with dot products\n\nY_manual_vec = np.zeros(shape = (N, n_neuron))\nfor x_idx in range(N):\n    x_x = x[x_idx]\n    for nu_idx in range(n_neuron):\n        w, b = W[nu_idx, :], B[nu_idx] # current W's shape needs to be transposed.\n        \n        z = torch.sum(x_x * w) + b\n        a = 1 / (1 + torch.exp(-z))\n        Y_manual_vec[x_idx, nu_idx] = a\n        \nprint('Y(with dot products): \\n', Y_manual_vec)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:37:07.017729Z","iopub.execute_input":"2022-06-10T11:37:07.018125Z","iopub.status.idle":"2022-06-10T11:37:07.035735Z","shell.execute_reply.started":"2022-06-10T11:37:07.018092Z","shell.execute_reply":"2022-06-10T11:37:07.034730Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Y(Torch): \n [[0.6461694  0.35609305 0.45774582]\n [0.49540743 0.4066069  0.548296  ]\n [0.4748421  0.17561656 0.38230848]\n [0.7100764  0.3594805  0.3363649 ]]\nY(Manual): \n [[0.6461694  0.35609305 0.45774582]\n [0.49540743 0.4066069  0.548296  ]\n [0.4748421  0.17561656 0.38230848]\n [0.7100764  0.35948047 0.3363649 ]]\nY(with dot products): \n [[0.64616942 0.35609305 0.45774582]\n [0.49540737 0.40660691 0.54829597]\n [0.4748421  0.17561658 0.38230848]\n [0.71007639 0.35948047 0.3363649 ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Cascaded Dense Layers","metadata":{}},{"cell_type":"markdown","source":"## Shapes of Cascaded Dense Layers","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\n\nN, n_feature = 4, 10\nX = torch.randn(N, n_feature)\n\nn_neurons = [3, 5]\n\nclass Dense(torch.nn.Module):\n    def __init__(self, n_neurons):\n        super(Dense, self).__init__()\n        self.layer1 = nn.Linear(10, n_neurons[0])\n        self.sigmoid = nn.Sigmoid()  \n        self.layer2 = nn.Linear(n_neurons[0], n_neurons[1])\n\n        \n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.sigmoid(x)\n        x = self.layer2(x)\n        x = self.sigmoid(x)\n        return x\n    \nmodel = Dense(n_neurons)\ny = model(X)\n\nw = list(model.parameters()) # get the values of weight, bias\nW1, W2 = w[0], w[2]\nB1, B2 = w[1], w[3]\n\nprint('Shape of W1: ', W1.shape)\nprint('Shape of B1: ', B1.shape)\nprint('Shape of W2: ', W2.shape)\nprint('Shape of B2: ', B2.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:08:49.048043Z","iopub.execute_input":"2022-06-10T11:08:49.048444Z","iopub.status.idle":"2022-06-10T11:08:49.061891Z","shell.execute_reply.started":"2022-06-10T11:08:49.048409Z","shell.execute_reply":"2022-06-10T11:08:49.060753Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Shape of W1:  torch.Size([3, 10])\nShape of B1:  torch.Size([3])\nShape of W2:  torch.Size([5, 3])\nShape of B2:  torch.Size([5])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Output Calculations","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\n\nN, n_feature = 4, 10\nX = torch.randn(N, n_feature)\n\nn_neurons = [3, 5]\n\nclass Dense(torch.nn.Module):\n    def __init__(self, n_neurons):\n        super(Dense, self).__init__()\n        self.layer1 = nn.Linear(10, n_neurons[0])\n        self.sigmoid = nn.Sigmoid()  \n        self.layer2 = nn.Linear(n_neurons[0], n_neurons[1])\n\n        \n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.sigmoid(x)\n        x = self.layer2(x)\n        x = self.sigmoid(x)\n        return x\n    \nmodel = Dense(n_neurons)\nY = model(X)\nprint('Y(Torch): \\n', Y.detach().numpy())\n\n\n\nw = list(model.parameters()) # get the values of weight, bias\nW1, W2 = w[0], w[2] # 3 x 10, 5 x 3\nB1, B2 = w[1], w[3] # 1 x 3, 1 x 5\n\nz1 = torch.matmul(X, torch.transpose(W1, 0, 1)) + B1 # 4 x 3\na1 = 1 / (1 + torch.exp(-z1))\nz2 = torch.matmul(a1, torch.transpose(W2, 0, 1)) + B2\nY_manual = 1 / (1 + torch.exp(-z2))\n\n    \nprint('\\nY(Manual): ', Y_manual.detach().numpy())\n","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:33:41.239509Z","iopub.execute_input":"2022-06-10T11:33:41.239922Z","iopub.status.idle":"2022-06-10T11:33:41.255925Z","shell.execute_reply.started":"2022-06-10T11:33:41.239889Z","shell.execute_reply":"2022-06-10T11:33:41.254808Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Y(Torch): \n [[0.590078   0.531353   0.40341693 0.2722073  0.32825536]\n [0.58798844 0.54451716 0.4135065  0.3291468  0.3706317 ]\n [0.6203747  0.5452508  0.42362025 0.27643445 0.3312044 ]\n [0.6067483  0.53530705 0.39559206 0.2632394  0.3265294 ]]\n\nY(Manual):  [[0.590078   0.531353   0.40341693 0.2722073  0.32825536]\n [0.58798844 0.54451716 0.4135065  0.3291468  0.3706317 ]\n [0.6203747  0.5452508  0.42362025 0.27643445 0.3312044 ]\n [0.6067483  0.53530705 0.39559206 0.2632394  0.3265294 ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}