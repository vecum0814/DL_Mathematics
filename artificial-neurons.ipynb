{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Activation Functions","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nimport numpy as np\n\n\nx = torch.randn(1, 5)\nprint(x)\n\n# list of activation functions\nsigmoid = nn.Sigmoid()\ntanh = nn.Tanh()\nrelu = nn.ReLU()\n\n# forward propagation(Torch)\ny_sig = sigmoid(x)\ny_tanh = tanh(x)\ny_relu = relu(x)\n\n# implementation on manual\ny_sig_impl = 1 / (1 + torch.exp(-x))\ny_tanh_impl = (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\ny_relu_impl = x * (x > 0)\n\n\nprint('Sigmoid(Torch): {}\\n{}'.format(y_sig.shape, y_sig.numpy()))\nprint('Tanh(Torch): {}\\n{}'.format(y_tanh.shape, y_tanh.numpy()))\nprint('ReLU(Torch): {}\\n{}'.format(y_relu.shape, y_relu.numpy()))\n\nprint('\\n-----Implemented Activation Functions Result----')\nprint('Sigmoid(impl): {}\\n{}'.format(y_sig_impl.shape, y_sig_impl.numpy()))\nprint('Tanh(impl): {}\\n{}'.format(y_tanh_impl.shape, y_tanh_impl.numpy()))\nprint('ReLU(impl): {}\\n{}'.format(y_relu_impl.shape, y_relu_impl.numpy()))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-07T13:04:40.781526Z","iopub.execute_input":"2022-06-07T13:04:40.782049Z","iopub.status.idle":"2022-06-07T13:04:40.797435Z","shell.execute_reply.started":"2022-06-07T13:04:40.781997Z","shell.execute_reply":"2022-06-07T13:04:40.796400Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"tensor([[-0.0255,  1.8212, -0.5620, -0.5810,  2.5007]])\nSigmoid(Torch): torch.Size([1, 5])\n[[0.49363673 0.86071575 0.36307925 0.35871354 0.92419386]]\nTanh(Torch): torch.Size([1, 5])\n[[-0.02544876  0.9489626  -0.5094777  -0.5233571   0.986634  ]]\nReLU(Torch): torch.Size([1, 5])\n[[0.        1.8212472 0.        0.        2.500743 ]]\n\n-----Implemented Activation Functions Result----\nSigmoid(impl): torch.Size([1, 5])\n[[0.49363673 0.86071575 0.36307925 0.35871354 0.92419386]]\nTanh(impl): torch.Size([1, 5])\n[[-0.02544878  0.9489626  -0.50947773 -0.5233571   0.98663414]]\nReLU(impl): torch.Size([1, 5])\n[[-0.         1.8212472 -0.        -0.         2.500743 ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Activation in Dense Layer","metadata":{}},{"cell_type":"code","source":"x = torch.randn(1, 10)\nprint(x.shape, '\\n', x)\n\nclass dense(nn.Module): # implementation of an affine function\n    def __init__(self, x):\n        super().__init__()\n        self.out = nn.Linear(x.shape[1], 1)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.out(x)\n        x = self.sig(x)\n        return x\n    \n    \n    \ny = dense(x) # forward propagation + parameter initialisation.\nw = list(y.parameters()) # get the values of weight, bias\n\n\nweight = w[0]\nbias = w[1]\n\ntmp = torch.transpose(weight, 0, 1)\n\n\n\nres = torch.matmul(x, tmp) + bias\nafter_sig = 1 / (1 + torch.exp(-res))\n\n\n\nprint('------Outputs-------')\nprint('Result(Torch) :', y(x).item())\nprint('Result(Impl) :', after_sig.item())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-07T13:51:46.068649Z","iopub.execute_input":"2022-06-07T13:51:46.069073Z","iopub.status.idle":"2022-06-07T13:51:46.080464Z","shell.execute_reply.started":"2022-06-07T13:51:46.069040Z","shell.execute_reply":"2022-06-07T13:51:46.079716Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"torch.Size([1, 10]) \n tensor([[ 0.0335, -0.1974,  0.9946,  0.5272,  0.6421, -0.6822, -2.6129,  0.2763,\n          0.5373,  1.2876]])\n------Outputs-------\nResult(Torch) : 0.36683595180511475\nResult(Impl) : 0.36683595180511475\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"x = torch.randn(1, 10)\n#print(x.shape, '\\n', x)\n\nclass dense(nn.Module): \n    def __init__(self, x):\n        super(dense, self).__init__()\n        self.out = nn.Linear(x.shape[1], 1)\n        \n    def forward(self, x):\n        x = self.out(x)\n        return x\n    \n    \n\n#model= dense(x)\n#y = model(x)\n\ny = dense(x)(x)\n\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T13:49:08.059488Z","iopub.execute_input":"2022-06-07T13:49:08.059873Z","iopub.status.idle":"2022-06-07T13:49:08.068240Z","shell.execute_reply.started":"2022-06-07T13:49:08.059837Z","shell.execute_reply":"2022-06-07T13:49:08.067542Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"tensor([[0.3609]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Minibatches","metadata":{}},{"cell_type":"markdown","source":"## Shapes of Linear Layer","metadata":{}},{"cell_type":"code","source":"N, n_features = 8, 10 # set input parameters\nx = torch.randn(N, n_features) # generate minibatch\nprint(x.shape)\n\nclass Linear(nn.Module): \n    def __init__(self):\n        super(Linear, self).__init__()\n        self.out = nn.Linear(10, 1)\n        self.relu = nn.ReLU(inplace = True)\n        \n    def forward(self, x):\n        x = self.out(x)\n        x = self.relu(x)\n        return x\n    \nmodel = Linear()\ny = model(x)\n\nw = list(model.parameters()) # get the values of weight, bias\nweight = w[0]\nbias = w[1]\n\nprint('Shape of x: {}', x.shape)\nprint('Shape of w: {}', weight.shape)\nprint('Shape of b: {}', bias.shape)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-07T14:01:43.239888Z","iopub.execute_input":"2022-06-07T14:01:43.240265Z","iopub.status.idle":"2022-06-07T14:01:43.249783Z","shell.execute_reply.started":"2022-06-07T14:01:43.240234Z","shell.execute_reply":"2022-06-07T14:01:43.249092Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"torch.Size([8, 10])\nShape of x: {} torch.Size([8, 10])\nShape of w: {} torch.Size([1, 10])\nShape of b: {} torch.Size([1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Output Calculation","metadata":{}},{"cell_type":"code","source":"N, n_features = 8, 10 # set input parameters\nx = torch.randn(N, n_features) # generate minibatch\n\nclass Linear(nn.Module): \n    def __init__(self):\n        super(Linear, self).__init__()\n        self.out = nn.Linear(10, 1)\n        self.relu = nn.ReLU(inplace = True)\n        \n    def forward(self, x):\n        x = self.out(x)\n        x = self.relu(x)\n        return x\n    \nmodel = Linear()\ny = model(x)\n\nw = list(model.parameters()) # get the values of weight, bias\nweight = w[0]\nbias = w[1]\n\ny_impl = torch.matmul(x, torch.transpose(weight, 0, 1)) + bias\ny_impl = y_impl * (y_impl > 0)\n\nprint('Result(Torch): ', y.detach().numpy())\nprint('Result(Impl): ', y_impl)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-07T14:09:20.793183Z","iopub.execute_input":"2022-06-07T14:09:20.793565Z","iopub.status.idle":"2022-06-07T14:09:20.805217Z","shell.execute_reply.started":"2022-06-07T14:09:20.793533Z","shell.execute_reply":"2022-06-07T14:09:20.804126Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Result(Torch):  [[0.40047273]\n [0.3695443 ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]]\nResult(Impl):  tensor([[0.4005],\n        [0.3695],\n        [-0.0000],\n        [-0.0000],\n        [-0.0000],\n        [-0.0000],\n        [-0.0000],\n        [-0.0000]], grad_fn=<MulBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}